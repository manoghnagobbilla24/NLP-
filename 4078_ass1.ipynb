{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOat5kupQ5KyiPbi83JzsK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manoghnagobbilla24/NLP-/blob/main/4078_ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsvI8B_Y2Dzs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce171dc1"
      },
      "source": [
        "# Task\n",
        "Perform text preprocessing by installing NLTK and spaCy, then load a text paragraph, count its words, and convert it to lowercase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f65e48"
      },
      "source": [
        "## Install and Import Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the NLTK and spaCy libraries using pip, then import them for NLP tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43ff5934"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to install the `nltk` library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a783a408",
        "outputId": "b375275d-fc5f-4224-854f-cefad3004262"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff5c75b1"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to import the `nltk` library, which was just installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0aba12b",
        "outputId": "fceb3dd8-949d-4b4f-f5a8-cc165ed8ff4f"
      },
      "source": [
        "import nltk\n",
        "print(\"NLTK library imported.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK library imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "843cc535"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the subtask instructions, the next step is to install the `spacy` library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46a117c4",
        "outputId": "a235b2ec-9754-48b2-8d5d-2d15d65f0f60"
      },
      "source": [
        "pip install spacy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd3e8cdc"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the subtask instructions, the next step is to download the small English language model for spaCy using `spacy.cli.download`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e64fd3ac",
        "outputId": "dd882a1f-e227-4bfa-a287-d959ee8f3852"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cf226c6"
      },
      "source": [
        "**Reasoning**:\n",
        "The final instruction in the subtask is to import the `spacy` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52e81649",
        "outputId": "11cb89d3-41c0-45a3-955d-f71bd0175457"
      },
      "source": [
        "import spacy\n",
        "print(\"spaCy library imported.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy library imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf7f651"
      },
      "source": [
        "## Load Text Paragraph\n",
        "\n",
        "### Subtask:\n",
        "Define a short paragraph of text within a Python string variable in the notebook. This will serve as the input for the preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553b05cd"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the subtask, the first step is to define a text paragraph as a Python string variable. This text will be used for all subsequent NLP operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84cae027"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, define the text paragraph as a Python string variable as instructed and print it to verify its content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61bb3bbb",
        "outputId": "f0cbe3fc-6577-43ef-9e86-78e9b6d233ae"
      },
      "source": [
        "text_paragraph = 'Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, process, and generate human language. It combines computational linguistics—rule-based modeling of human language—with machine learning, deep learning, and AI models.'\n",
        "print(text_paragraph)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, process, and generate human language. It combines computational linguistics—rule-based modeling of human language—with machine learning, deep learning, and AI models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd0049fe"
      },
      "source": [
        "## Count Words\n",
        "\n",
        "### Subtask:\n",
        "Write Python code to count the total number of words in the loaded text paragraph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4761428"
      },
      "source": [
        "As per the subtask, the next step is to write Python code to count the total number of words in the `text_paragraph`. This involves splitting the string into words and then counting the elements in the resulting list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae8c5732"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the detailed instructions, I will now write Python code to split the `text_paragraph` into words, count them, and store the result in `word_count`, then print it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e661f952",
        "outputId": "01a95d33-8cae-4811-d958-496540295106"
      },
      "source": [
        "words = text_paragraph.split()\n",
        "word_count = len(words)\n",
        "print(f\"Total word count: {word_count}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80262d2c"
      },
      "source": [
        "## Convert to Lowercase\n",
        "\n",
        "### Subtask:\n",
        "Apply Python's built-in string methods to convert the entire loaded text paragraph to lowercase. This step demonstrates a basic text normalization technique.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88f013ee"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the subtask, the `text_paragraph` needs to be converted to lowercase. Python's built-in string method `.lower()` is suitable for this. The result will then be stored in a new variable, `lowercase_text`, and printed to confirm the conversion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09b018ef"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the subtask instructions, I will convert the `text_paragraph` to lowercase using the `.lower()` string method, store it in `lowercase_text`, and then print the result to verify the conversion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e49d62cc",
        "outputId": "d0181888-3bcd-4201-ae87-dceb59fc7383"
      },
      "source": [
        "lowercase_text = text_paragraph.lower()\n",
        "print(lowercase_text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language processing (nlp) is a field of artificial intelligence that focuses on enabling computers to understand, process, and generate human language. it combines computational linguistics—rule-based modeling of human language—with machine learning, deep learning, and ai models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ca399af"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the NLP preprocessing steps performed, including the original text, the total word count, and the lowercase version of the text, along with a note on how to submit the notebook as a PDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd99337"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The NLP preprocessing steps performed include:\n",
        "*   **Original Text**: \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, process, and generate human language. It combines computational linguistics—rule-based modeling of human language—with machine learning, deep learning, and AI models.\"\n",
        "*   **Total Word Count**: The original text contains 37 words.\n",
        "*   **Lowercase Version of the Text**: \"natural language processing (nlp) is a field of artificial intelligence that focuses on enabling computers to understand, process, and generate human language. it combines computational linguistics—rule-based modeling of human language—with machine learning, deep learning, and ai models.\"\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   NLTK and spaCy libraries were successfully installed and imported, confirming readiness for NLP tasks. The `en_core_web_sm` spaCy language model was also downloaded.\n",
        "*   A text paragraph was defined as a Python string for subsequent processing.\n",
        "*   The total word count of the original text was determined to be 37 words by splitting the text and counting the resulting elements.\n",
        "*   The entire text paragraph was successfully converted to lowercase using the `.lower()` string method.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The current preprocessing steps form a solid foundation; further NLP techniques such as tokenization, stop-word removal, stemming, or lemmatization could be applied to prepare the text for more advanced analysis or machine learning models.\n",
        "*   Explore more sophisticated text normalization by handling punctuation, special characters, and numbers, which were not explicitly addressed in this initial lowercase conversion.\n"
      ]
    }
  ]
}